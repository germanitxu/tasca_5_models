{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff723803",
   "metadata": {},
   "source": [
    "\n",
    "## 1. Setup\n",
    "El kernel del meu notebook, que no esta al colab, te la versió 3.9.0 de Keras y 2.19.0 de Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "39c5f72c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 22:25:34.000390: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-17 22:25:34.000758: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 22:25:34.002993: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-03-17 22:25:34.009537: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1742246734.020224  794600 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1742246734.023348  794600 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1742246734.031463  794600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742246734.031472  794600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742246734.031473  794600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1742246734.031473  794600 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-03-17 22:25:34.034432: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# We set the backend to TensorFlow. The code works with\n",
    "# both `tensorflow` and `torch`. It does not work with JAX\n",
    "# due to the behavior of `jax.numpy.tile` in a jit scope\n",
    "# (used in `causal_attention_mask():`) `tile` in JAX does\n",
    "# not support a dynamic `reps` argument.\n",
    "# You can make the code work in JAX by wrapping the\n",
    "# inside of the `causal_attention_mask` function in\n",
    "# a decorator to prevent jit compilation:\n",
    "# `with jax.ensure_compile_time_eval():`.\n",
    "import os\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "import keras\n",
    "import numpy as np\n",
    "import tensorflow\n",
    "import tensorflow.data as tf_data\n",
    "import tensorflow.strings as tf_strings\n",
    "from keras import layers, ops\n",
    "\n",
    "TextVectorization = layers.TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dc9ff9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "## 2. Implementació d'un bloc transformer com a capa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c1a0d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def causal_attention_mask(batch_size, n_dest, n_src, dtype):\n",
    "    \"\"\"\n",
    "    Mask the upper half of the dot product matrix in self attention.\n",
    "    This prevents flow of information from future tokens to current token.\n",
    "    1's in the lower triangle, counting from the lower right corner.\n",
    "    \"\"\"\n",
    "    i = ops.arange(n_dest)[:, None]\n",
    "    j = ops.arange(n_src)\n",
    "    m = i >= j - n_src + n_dest\n",
    "    mask = ops.cast(m, dtype)\n",
    "    mask = ops.reshape(mask, [1, n_dest, n_src])\n",
    "    mult = ops.concatenate(\n",
    "        [ops.expand_dims(batch_size, -1), ops.convert_to_tensor([1, 1])], 0\n",
    "    )\n",
    "    return ops.tile(mask, mult)\n",
    "\n",
    "\n",
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super().__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads, embed_dim)\n",
    "        self.ffn = keras.Sequential(\n",
    "            [\n",
    "                layers.Dense(ff_dim, activation=\"relu\"),\n",
    "                layers.Dense(embed_dim),\n",
    "            ]\n",
    "        )\n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = ops.shape(inputs)\n",
    "        batch_size = input_shape[0]\n",
    "        seq_len = input_shape[1]\n",
    "        causal_mask = causal_attention_mask(batch_size, seq_len, seq_len, \"bool\")\n",
    "        attention_output = self.att(inputs, inputs, attention_mask=causal_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "        out1 = self.layernorm1(inputs + attention_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6b5752-0ef0-42c0-bb81-2f86915ab281",
   "metadata": {},
   "source": [
    "**Què significa que la màscara d'atenció sigui causal?**\n",
    "\n",
    "Extret del comentari de la funció, bloqueja que cada token pugui veure tokens posteriors i asegura que les prediccions\n",
    "només depenguin de tokens anteriors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9871a3cd",
   "metadata": {},
   "source": [
    "## 3. Implementació de les capes d'embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2822134b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two separate embedding layers: one for tokens and one for token index (positions).\n",
    "class TokenAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super().__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = ops.shape(x)[-1]\n",
    "        positions = ops.arange(0, maxlen, 1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45bc6dc",
   "metadata": {},
   "source": [
    "## 4. Implementació del GPT en miniatura.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3875df6",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 20000  # Only consider the top 20k words\n",
    "maxlen = 80  # Max sequence size\n",
    "embed_dim = 256  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "feed_forward_dim = 256  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "\n",
    "def create_model():\n",
    "    inputs = layers.Input(shape=(maxlen,), dtype=\"int32\")\n",
    "    embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "    x = embedding_layer(inputs)\n",
    "    transformer_block = TransformerBlock(embed_dim, num_heads, feed_forward_dim)\n",
    "    x = transformer_block(x)\n",
    "    outputs = layers.Dense(vocab_size)(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=[outputs, x])\n",
    "    loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "    model.compile(\n",
    "        \"adam\",\n",
    "        loss=[loss_fn, None],\n",
    "    )  # No loss and optimization based on word embeddings from transformer block\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6170b49-cb34-40cb-827e-435d311a63a9",
   "metadata": {},
   "source": [
    "**Quines funcions implementades als punts anteriors s'invoquen ara?**\n",
    "* TokenAndPositionEmbedding (Capa d'embedding)\n",
    "* TransformerBlock (Bloc transformer com a capa)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d044f9ae",
   "metadata": {},
   "source": [
    "## 5. Dades per al model de llenguatge a nivell de paraula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e16ec86-59e0-458f-9abd-39f01311a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: data\n",
      "Warning: Binary output can mess up your terminal. Use \"--output -\" to tell \n",
      "Warning: curl to output it to your terminal anyway, or consider \"--output \n",
      "Warning: <FILE>\" to save to a file.\n",
      "tar: data: Not found in archive\n",
      "tar: Exiting with failure status due to previous errors\n"
     ]
    }
   ],
   "source": [
    "# Download the IMDB dataset and combine training and validation sets for a text generation task\n",
    "!curl -O data/aclImdb_v1.tar.gz https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf data/aclImdb_v1.tar.gz data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3385074b-f1af-4cde-9b3b-a5b8852f6476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 files\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E0000 00:00:1742246738.798421  794600 cuda_executor.cc:1228] INTERNAL: CUDA Runtime error: Failed call to cudaGetRuntimeVersion: Error loading CUDA libraries. GPU will not be used.: Error loading CUDA libraries. GPU will not be used.\n",
      "W0000 00:00:1742246738.798672  794600 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n",
      "2025-03-17 22:25:41.104592: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# The dataset contains each review in a separate text file\n",
    "# The text files are present in four different folders\n",
    "# Create a list all files\n",
    "filenames = []\n",
    "directories = [\n",
    "    \"data/aclImdb/train/pos\",\n",
    "    \"data/aclImdb/train/neg\",\n",
    "    \"data/aclImdb/test/pos\",\n",
    "    \"data/aclImdb/test/neg\",\n",
    "]\n",
    "for dir in directories:\n",
    "    for f in os.listdir(dir):\n",
    "        filenames.append(os.path.join(dir, f))\n",
    "\n",
    "print(f\"{len(filenames)} files\")\n",
    "\n",
    "# Create a dataset from text files\n",
    "random.shuffle(filenames)\n",
    "text_ds = tf_data.TextLineDataset(filenames)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
    "    lowercased = tf_strings.lower(input_string)\n",
    "    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\1\")\n",
    "\n",
    "\n",
    "# Create a vectorization layer and adapt it to the text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # To get words back from token indices\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Shift word sequences by 1 position so that the target for position (i) is\n",
    "    word at position (i+1). The model will use all words up till position (i)\n",
    "    to predict the next word.\n",
    "    \"\"\"\n",
    "    text = tensorflow.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80ccc98-632b-4d09-9d90-83fcd909b96d",
   "metadata": {},
   "source": [
    "**Quina és l'aplicació habitual del dataset Imdb?**\n",
    "\n",
    "Segons el README.md del zip descarregat, el dataset es pot utilitzar per entrenar models per classificar un comentari en un sentiment positiu o negatiu al que es comenta."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619e4c1-3767-4aa8-b8f7-b4df77b00892",
   "metadata": {},
   "source": [
    "## 6. Implementació del callback Keras per generar text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "281589ec-8f5c-4e04-97b3-0f6c9075abe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, max_tokens, start_tokens, index_to_word, top_k=10, print_every=1\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_token_list = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        while num_tokens_generated <= self.max_tokens:\n",
    "            pad_len = maxlen - len(start_token_list)\n",
    "            sample_index = len(start_token_list) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_token_list[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_token_list + [0] * pad_len\n",
    "            else:\n",
    "                x = start_token_list\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_token_list.append(sample_token)\n",
    "            num_tokens_generated = len(tokens_generated)\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"this movie is\"\n",
    "start_tokens = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "total_tokens_generated = 40\n",
    "text_gen_callback = TextGenerator(total_tokens_generated, start_tokens, vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3d195c-3567-4fe4-acf3-022ec8eef623",
   "metadata": {},
   "source": [
    "**Quin canvi faríeu al codi perquè sempre triàs la paraula més probable?**\n",
    "\n",
    "Sembla que a `sample_from`, quan es tria el token, ho fa de forma aleatoria amb `np.random.choice(indices, p=preds)`. Si `logits` representa un llistat de tokens amb la seva probabilitat, retornaria el valor màxim d'aquest llistat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2030ef6-065d-48fa-bdd2-8fe23d19d58d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from(self, logits):\n",
    "    return np.argmax(logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b78fdb7-86d4-4422-b1e2-7044feb8779f",
   "metadata": {},
   "source": [
    "## 7. Entrenau el model amb un altre dataset de text d'una mida suficient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d35bd64-864b-406d-a42f-73fa6aacf4a0",
   "metadata": {},
   "source": [
    "He escollit un dataset de ressenyes de productes dAmazon. A Kaggle hi ha força datasets disponibles, però són tots massa pesats, oferint molts més textos dels que necessitem (en la magnitud del milió de reviews per arxiu).\n",
    "\n",
    "He trobat [una web](https://amazon-reviews-2023.github.io/#grouped-by-category) on s'ofereix ressenyes en format `.jsonl`, subcategoritzades, reduint així el nombre de ressenyes a unes cent mil (les més lleugeres)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9fb0c03f-8a0e-41ed-8721-d28555976a56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: data\n",
      "Warning: Binary output can mess up your terminal. Use \"--output -\" to tell \n",
      "Warning: curl to output it to your terminal anyway, or consider \"--output \n",
      "Warning: <FILE>\" to save to a file.\n",
      "tar: This does not look like a tar archive\n",
      "tar: Skipping to next header\n",
      "tar: data/Magazine_Subscriptions.jsonl: Not found in archive\n",
      "tar: Exiting with failure status due to previous errors\n"
     ]
    }
   ],
   "source": [
    "!curl -O data/Magazine_Subscriptions.jsonl.gz https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/review_categories/Magazine_Subscriptions.jsonl.gz\n",
    "!tar -xf data/Magazine_Subscriptions.jsonl.gz data/Magazine_Subscriptions.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dae7dea0-fbed-415f-a4bf-727cd63f9d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-17 22:25:44.363082: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: OUT_OF_RANGE: End of sequence\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Carrega les dades des del DataFrame de Pandas\n",
    "def load_texts_from_dataframe(df, text_column):\n",
    "    \"\"\"Extracts texts from a pandas DataFrame column.\"\"\"\n",
    "    try:\n",
    "        texts = df[text_column].astype(str).tolist()  # Ensure texts are strings\n",
    "        return texts\n",
    "    except KeyError:\n",
    "        raise KeyError(f\"Column '{text_column}' not found in DataFrame.\")\n",
    "\n",
    "\n",
    "# Carrega les dades des del DataFrame\n",
    "jsonObj = pd.read_json(path_or_buf=\"data/Magazine_Subscriptions.jsonl\", lines=True)\n",
    "try:\n",
    "    texts = load_texts_from_dataframe(jsonObj, \"text\")\n",
    "except KeyError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    exit()\n",
    "\n",
    "\n",
    "# Crea un dataset de TensorFlow a partir dels textos\n",
    "text_ds = tf_data.Dataset.from_tensor_slices(texts)\n",
    "text_ds = text_ds.shuffle(buffer_size=256)\n",
    "text_ds = text_ds.batch(batch_size)\n",
    "\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    \"\"\"Remove html line-break tags and handle punctuation\"\"\"\n",
    "    lowercased = tf_strings.lower(input_string)\n",
    "    stripped_html = tf_strings.regex_replace(lowercased, \"<br />\", \" \")\n",
    "    return tf_strings.regex_replace(stripped_html, f\"([{string.punctuation}])\", r\" \\\\1\")\n",
    "\n",
    "\n",
    "# Crea una capa de vectorització i adapta-la al text\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize=custom_standardization,\n",
    "    max_tokens=vocab_size - 1,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=maxlen + 1,\n",
    ")\n",
    "vectorize_layer.adapt(text_ds)\n",
    "vocab = vectorize_layer.get_vocabulary()  # Per recuperar paraules a partir dels índexs\n",
    "\n",
    "\n",
    "def prepare_lm_inputs_labels(text):\n",
    "    \"\"\"\n",
    "    Desplaça les seqüències de paraules per 1 posició, de manera que l'objectiu per a la posició (i) sigui\n",
    "    la paraula a la posició (i+1). El model utilitzarà totes les paraules fins a la posició (i)\n",
    "    per predir la paraula següent.\n",
    "    \"\"\"\n",
    "    text = tensorflow.expand_dims(text, -1)\n",
    "    tokenized_sentences = vectorize_layer(text)\n",
    "    x = tokenized_sentences[:, :-1]\n",
    "    y = tokenized_sentences[:, 1:]\n",
    "    return x, y\n",
    "\n",
    "\n",
    "text_ds = text_ds.map(prepare_lm_inputs_labels, num_parallel_calls=tf_data.AUTOTUNE)\n",
    "text_ds = text_ds.prefetch(tf_data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "58d544ea-3a29-4ad0-b938-070a747c0dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model()\n",
    "# Hem de redefinir els tokens d'entrada, pero que sigués coherent amb les dades\n",
    "start_prompt = \"this magazine is\"\n",
    "start_tokens_list = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "total_tokens_generated = 40\n",
    "text_gen_callback = TextGenerator(total_tokens_generated, start_tokens_list, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0c153812-6534-4dc4-b45d-929ee767af0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epochs = 25\n",
    "epochs = 1  # No tením tot el dia\n",
    "# model.fit(text_ds, verbose=2, epochs=epochs, callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ffefea-eb3d-46c5-a1ca-d372d6f1d5ec",
   "metadata": {},
   "source": [
    "## 8. Canviau el codi de generació de text, de forma que en lloc d'aturar quan ha generat un nombre de tokens, aturi quan genera un punt. D'aquesta forma, les frases generades sempre seran completes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9bb04171-be7f-45ca-bc8e-128aae457acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator(keras.callbacks.Callback):\n",
    "    \"\"\"A callback to generate text from a trained model.\n",
    "    1. Feed some starting prompt to the model\n",
    "    2. Predict probabilities for the next token\n",
    "    3. Sample the next token and add it to the next input\n",
    "\n",
    "    Arguments:\n",
    "        max_tokens: Integer, the number of tokens to be generated after prompt.\n",
    "        start_tokens: List of integers, the token indices for the starting prompt.\n",
    "        index_to_word: List of strings, obtained from the TextVectorization layer.\n",
    "        top_k: Integer, sample from the `top_k` token predictions.\n",
    "        print_every: Integer, print after this many epochs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_tokens,\n",
    "        start_tokens,\n",
    "        interrupt_token,\n",
    "        index_to_word,\n",
    "        top_k=10,\n",
    "        print_every=1,\n",
    "    ):\n",
    "        self.max_tokens = max_tokens\n",
    "        self.start_tokens = start_tokens\n",
    "        self.interrupt_token = interrupt_token\n",
    "        self.index_to_word = index_to_word\n",
    "        self.print_every = print_every\n",
    "        self.k = top_k\n",
    "\n",
    "    def sample_from(self, logits):\n",
    "        logits, indices = ops.top_k(logits, k=self.k, sorted=True)\n",
    "        indices = np.asarray(indices).astype(\"int32\")\n",
    "        preds = keras.activations.softmax(ops.expand_dims(logits, 0))[0]\n",
    "        preds = np.asarray(preds).astype(\"float32\")\n",
    "        return np.random.choice(indices, p=preds)\n",
    "\n",
    "    def detokenize(self, number):\n",
    "        return self.index_to_word[number]\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        start_token_list = [_ for _ in self.start_tokens]\n",
    "        if (epoch + 1) % self.print_every != 0:\n",
    "            return\n",
    "        num_tokens_generated = 0\n",
    "        tokens_generated = []\n",
    "        # Si troba un . s'atura el while.\n",
    "        # Encara que s'hauria d'aturar quan trobi un punt, no podemo saber si ho generara, llavors mantenim tambe la\n",
    "        # condició anterior.\n",
    "        while (\n",
    "            self.interrupt_token not in tokens_generated\n",
    "            and num_tokens_generated <= self.max_tokens\n",
    "        ):\n",
    "            pad_len = maxlen - len(start_token_list)\n",
    "            sample_index = len(start_token_list) - 1\n",
    "            if pad_len < 0:\n",
    "                x = start_token_list[:maxlen]\n",
    "                sample_index = maxlen - 1\n",
    "            elif pad_len > 0:\n",
    "                x = start_token_list + [0] * pad_len\n",
    "            else:\n",
    "                x = start_token_list\n",
    "            x = np.array([x])\n",
    "            y, _ = self.model.predict(x, verbose=0)\n",
    "            sample_token = self.sample_from(y[0][sample_index])\n",
    "            tokens_generated.append(sample_token)\n",
    "            start_token_list.append(sample_token)\n",
    "            num_tokens_generated += len(tokens_generated)\n",
    "            print(f\"Generated {str(num_tokens_generated)} tokens\")\n",
    "\n",
    "        txt = \" \".join(\n",
    "            [self.detokenize(_) for _ in self.start_tokens + tokens_generated]\n",
    "        )\n",
    "        print(f\"generated text:\\n{txt}\\n\")\n",
    "\n",
    "\n",
    "# Tokenize starting prompt\n",
    "word_to_index = {}\n",
    "for index, word in enumerate(vocab):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "start_prompt = \"this magazine is\"\n",
    "period_token = word_to_index.get(\".\", 1)\n",
    "start_tokens_list = [word_to_index.get(_, 1) for _ in start_prompt.split()]\n",
    "total_tokens_generated = 10000\n",
    "text_gen_callback = TextGenerator(\n",
    "    total_tokens_generated, start_tokens_list, period_token, vocab\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3f70bb9-a993-40fb-a45d-6184f2d9f9d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m559/559\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2s/step - loss: 2.6044Generated 1 tokens\n",
      "Generated 3 tokens\n",
      "Generated 6 tokens\n",
      "Generated 10 tokens\n",
      "Generated 15 tokens\n",
      "Generated 21 tokens\n",
      "Generated 28 tokens\n",
      "Generated 36 tokens\n",
      "Generated 45 tokens\n",
      "Generated 55 tokens\n",
      "Generated 66 tokens\n",
      "Generated 78 tokens\n",
      "Generated 91 tokens\n",
      "Generated 105 tokens\n",
      "Generated 120 tokens\n",
      "Generated 136 tokens\n",
      "Generated 153 tokens\n",
      "Generated 171 tokens\n",
      "Generated 190 tokens\n",
      "Generated 210 tokens\n",
      "Generated 231 tokens\n",
      "Generated 253 tokens\n",
      "Generated 276 tokens\n",
      "Generated 300 tokens\n",
      "Generated 325 tokens\n",
      "Generated 351 tokens\n",
      "Generated 378 tokens\n",
      "Generated 406 tokens\n",
      "Generated 435 tokens\n",
      "Generated 465 tokens\n",
      "Generated 496 tokens\n",
      "Generated 528 tokens\n",
      "Generated 561 tokens\n",
      "Generated 595 tokens\n",
      "Generated 630 tokens\n",
      "Generated 666 tokens\n",
      "Generated 703 tokens\n",
      "Generated 741 tokens\n",
      "Generated 780 tokens\n",
      "Generated 820 tokens\n",
      "Generated 861 tokens\n",
      "Generated 903 tokens\n",
      "Generated 946 tokens\n",
      "Generated 990 tokens\n",
      "Generated 1035 tokens\n",
      "Generated 1081 tokens\n",
      "Generated 1128 tokens\n",
      "Generated 1176 tokens\n",
      "Generated 1225 tokens\n",
      "Generated 1275 tokens\n",
      "Generated 1326 tokens\n",
      "Generated 1378 tokens\n",
      "Generated 1431 tokens\n",
      "Generated 1485 tokens\n",
      "Generated 1540 tokens\n",
      "Generated 1596 tokens\n",
      "Generated 1653 tokens\n",
      "Generated 1711 tokens\n",
      "Generated 1770 tokens\n",
      "Generated 1830 tokens\n",
      "Generated 1891 tokens\n",
      "Generated 1953 tokens\n",
      "Generated 2016 tokens\n",
      "Generated 2080 tokens\n",
      "Generated 2145 tokens\n",
      "Generated 2211 tokens\n",
      "Generated 2278 tokens\n",
      "Generated 2346 tokens\n",
      "Generated 2415 tokens\n",
      "Generated 2485 tokens\n",
      "Generated 2556 tokens\n",
      "Generated 2628 tokens\n",
      "Generated 2701 tokens\n",
      "Generated 2775 tokens\n",
      "Generated 2850 tokens\n",
      "Generated 2926 tokens\n",
      "Generated 3003 tokens\n",
      "Generated 3081 tokens\n",
      "Generated 3160 tokens\n",
      "Generated 3240 tokens\n",
      "Generated 3321 tokens\n",
      "Generated 3403 tokens\n",
      "Generated 3486 tokens\n",
      "Generated 3570 tokens\n",
      "Generated 3655 tokens\n",
      "Generated 3741 tokens\n",
      "Generated 3828 tokens\n",
      "Generated 3916 tokens\n",
      "Generated 4005 tokens\n",
      "Generated 4095 tokens\n",
      "Generated 4186 tokens\n",
      "Generated 4278 tokens\n",
      "Generated 4371 tokens\n",
      "Generated 4465 tokens\n",
      "Generated 4560 tokens\n",
      "Generated 4656 tokens\n",
      "Generated 4753 tokens\n",
      "Generated 4851 tokens\n",
      "Generated 4950 tokens\n",
      "Generated 5050 tokens\n",
      "Generated 5151 tokens\n",
      "Generated 5253 tokens\n",
      "Generated 5356 tokens\n",
      "Generated 5460 tokens\n",
      "Generated 5565 tokens\n",
      "Generated 5671 tokens\n",
      "Generated 5778 tokens\n",
      "Generated 5886 tokens\n",
      "Generated 5995 tokens\n",
      "Generated 6105 tokens\n",
      "Generated 6216 tokens\n",
      "Generated 6328 tokens\n",
      "Generated 6441 tokens\n",
      "Generated 6555 tokens\n",
      "Generated 6670 tokens\n",
      "Generated 6786 tokens\n",
      "Generated 6903 tokens\n",
      "Generated 7021 tokens\n",
      "Generated 7140 tokens\n",
      "Generated 7260 tokens\n",
      "Generated 7381 tokens\n",
      "Generated 7503 tokens\n",
      "Generated 7626 tokens\n",
      "Generated 7750 tokens\n",
      "Generated 7875 tokens\n",
      "Generated 8001 tokens\n",
      "Generated 8128 tokens\n",
      "Generated 8256 tokens\n",
      "Generated 8385 tokens\n",
      "Generated 8515 tokens\n",
      "Generated 8646 tokens\n",
      "Generated 8778 tokens\n",
      "Generated 8911 tokens\n",
      "Generated 9045 tokens\n",
      "Generated 9180 tokens\n",
      "Generated 9316 tokens\n",
      "Generated 9453 tokens\n",
      "Generated 9591 tokens\n",
      "Generated 9730 tokens\n",
      "Generated 9870 tokens\n",
      "Generated 10011 tokens\n",
      "generated text:\n",
      "this magazine is a great resource \\1 i have not received my copy of this magazine \\1 i have been a long time and read this magazine to my son \\1 i \\1ve seen a month subscription and still have received my first two months later \\1 but i received an issue \\1 i received 3 and was a month for a subscription \\1                                                                                \n",
      "\n",
      "\u001b[1m559/559\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1129s\u001b[0m 2s/step - loss: 2.6034\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x70c5871e7530>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# epochs = 25\n",
    "epochs = 1\n",
    "model.fit(text_ds, verbose=1, epochs=epochs, callbacks=[text_gen_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dae5fa-fdeb-409e-aabb-21b1425ed26e",
   "metadata": {},
   "source": [
    "No troba cap punt, potser sigui perquè quan fem servir les dades, a `custom_standardization`, li traiem tots els punts als textos de les ressenyes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07eaeaf8-2b43-4287-ab74-fc2a913fecf8",
   "metadata": {},
   "source": [
    "## 9. Comparau el rànquing de xatbots disponible a lmarena.ai amb el dels apunts. Quines diferències hi destacau?\n",
    "\n",
    "Als apunts no s'esmenta grok, que actualment ocupa el primer lloc al rànquing. Pel que fa a la resta de chatbots, sembla que dels esmentats als apunts, Gemini, ChatGPT i Claude mantenen les seves posicions. A destacar Crida que ha augmentat considerablement la seva posició.\n",
    "El rànquing dels apunts estava dominat per models de Google i OpenAI, però a l'actual, està més diversificat, incloent-hi fins i tot models amb llicències d'ús obert."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9a1a3f-10e9-48d1-a12c-d732f6f8805c",
   "metadata": {},
   "source": [
    "## 10. Provau alguns LLM que hi ha disponibles a través de la interfície de xat de HuggingFace i comentau les diferències que hi heu observat. Hi ha models recents com DeepSeek 3 i Grok 3?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6213c0f3-818d-43f2-9fa9-2ec295ea27bc",
   "metadata": {},
   "source": [
    "He demanat el mateix prompt als models:\n",
    "\n",
    "**Si vull anar de Palma de Mallorca fins a Hyderabad amb cotxe, explica els requisits que necessito complir, com a documentació necessària, vacunes, zones a evitar i punts d'interès pels quals valguin la pena desviar-se del camí més curt i ràpid.**\n",
    "\n",
    "He desat les respostes al [directori de resposts d'aquest repo](respostes_models/README.md)\n",
    "\n",
    "\n",
    "### Llama\n",
    "Resposta balanceada y completa.  Fa mencio a tot les peticions fetas.\n",
    "\n",
    "### Qwen\n",
    "Resposta molt parescuda a la de Llama, pero te errors, com el llistat de paisos a visitar i el visats necessaris per viatjar a aquests països.\n",
    "\n",
    "### Deepkseek\n",
    "A meitat de resposta, el model comença a barrejar el text amb les etiquetes de markdown i l'idioma de la resposta amb altres.\n",
    "\n",
    "### Mistral\n",
    "Resposta molt curta i incompleta. Mistral falla a esmentar aspectes claus del llistat de peticions\n",
    "\n",
    "\n",
    "**Hi ha models recents com DeepSeek 3 i Grok 3?**\n",
    "\n",
    "Actualment Deepseek V3 no està disponible a la web, però si el model Deepseek R1, que és més modern, no es troba entre la llista de models disponibles."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "ipynb,py"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
